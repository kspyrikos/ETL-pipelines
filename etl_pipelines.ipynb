{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c353e0a-d7a0-497e-b2c9-0b1e5c08df52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "from io import StringIO, BytesIO\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4f2dcb-1b46-4a84-93ca-b29a184cdcc6",
   "metadata": {},
   "source": [
    "# Adapter Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82e33476-ac37-4faf-9790-7bf92dfbdc95",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def read_csv_to_df(bucket, key, decoding = 'utf-8', sep=','):\n",
    "    csv_obj = bucket.Object(key=key).get().get('Body').read().decode(decoding)\n",
    "    data = StringIO(csv_obj)\n",
    "    df = pd.read_csv(data, delimiter=sep)\n",
    "    return df\n",
    "\n",
    "def write_df_to_s3(bucket, df, key):\n",
    "    out_buffer = BytesIO()\n",
    "    df.to_parquet(out_buffer, index = False)\n",
    "    bucket.put_object(Body=out_buffer.getvalue(), Key=key)\n",
    "    return True\n",
    "\n",
    "def write_df_to_s3_csv(bucket, df, key):\n",
    "    out_buffer = StringIO()\n",
    "    df.to_csv(out_buffer, index = False)\n",
    "    bucket.put_object(Body=out_buffer.getvalue(), Key=key)\n",
    "    return True\n",
    "\n",
    "# def return_objects(bucket, arg_date, src_format):\n",
    "#     min_date = datetime.strptime(arg_date, src_format).date() - timedelta(days=1)\n",
    "#     max_date =  datetime.strptime(arg_date, src_format).date() + timedelta(days=7) # I get the data of one week (7 days)\n",
    "#     objects = [obj for obj in bucket.objects.all() if datetime.strptime(obj.key.split('/')[0], src_format).date() >= min_date and datetime.strptime(obj.key.split('/')[0], src_format).date() < max_date] #filter objects of the bucket\n",
    "#     #print(objects)\n",
    "#     return objects   \n",
    "\n",
    "def list_files_in_prefix(bucket, prefix):\n",
    "    files = [obj.key for obj in bucket.objects.filter(Prefix=prefix)]\n",
    "    return files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d78f798-8d6c-4388-8706-18d8541254c9",
   "metadata": {},
   "source": [
    "# Application Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94dde3ac-aa85-4d15-8cbb-1112b37c711e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(bucket, date_list):\n",
    "    print(date_list)\n",
    "    files = [key for date in date_list for key in list_files_in_prefix(bucket, date)]\n",
    "    df = pd.concat([read_csv_to_df(bucket, obj) for obj in files], ignore_index=True)\n",
    "    return df\n",
    "    \n",
    "def transform_report1(df, columns, arg_date):\n",
    "    df = df.loc[:, columns]\n",
    "    df.dropna(inplace = True)\n",
    "    df['opening_price'] = df.sort_values(by=['Time']).groupby(['ISIN','Date'])['StartPrice'].transform('first')\n",
    "    df['closing_price'] = df.sort_values(by=['Time']).groupby(['ISIN','Date'])['EndPrice'].transform('last')\n",
    "    df = df.groupby(['ISIN', 'Date'], as_index = False).agg(opening_price_eur=('opening_price', 'min'), closing_price_eur=('closing_price', 'min'), minimum_price_eur=('MinPrice', 'min'), maximum_price_eur=('MaxPrice', 'max'), daily_traded_volume=('TradedVolume', 'sum'))\n",
    "    df['prev_closing_price'] = df.sort_values(by=['Date']).groupby(['ISIN'])['closing_price_eur'].shift(1)\n",
    "    df['change_prev_closing_%'] = (df['closing_price_eur'] - df['prev_closing_price']) / df['prev_closing_price']*100\n",
    "    df.drop(columns=['prev_closing_price'], inplace=True)\n",
    "    df = df.round(decimals = 2)\n",
    "    df = df[df.Date >= arg_date] # we filter from the arg_date forward\n",
    "    return df\n",
    "\n",
    "def load(bucket, df, trg_key, trg_format, meta_key, extract_date_list):\n",
    "    key = trg_key + datetime.today().strftime(\"%Y%m%d_%H%M%S\") + trg_format\n",
    "    write_df_to_s3(bucket, df, key)\n",
    "    update_meta_file(bucket, meta_key, extract_date_list)\n",
    "    return True\n",
    "\n",
    "def etl_report1(src_bucket, trg_bucket, date_list, columns, arg_date, trg_key, trg_format, meta_key):\n",
    "    df = extract(src_bucket, date_list)\n",
    "    df = transform_report1(df, columns, arg_date)\n",
    "    extract_date_list = [date for date in date_list if date >= arg_date]\n",
    "    load(trg_bucket, df, trg_key, trg_format, meta_key, extract_date_list)\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776207fb-5e0c-4abf-8ce1-60d41ee5de5a",
   "metadata": {},
   "source": [
    "# Application Layer - not Core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "87c5154f-c9fa-44b3-aa16-af31168ab9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_date_list(bucket, arg_date, src_format, meta_key):\n",
    "    min_date = datetime.strptime(arg_date, src_format).date() - timedelta(days=1)\n",
    "    max_date = datetime.strptime(arg_date, src_format).date() + timedelta(days=7)\n",
    "    print(min_date, max_date)\n",
    "    #today = datetime.today().date()\n",
    "    try:\n",
    "        df_meta = read_csv_to_df(bucket, meta_key)\n",
    "        dates = [(min_date + timedelta(days=x)) for x in range(0, (max_date-min_date).days + 1)]\n",
    "        src_dates = set(pd.to_datetime(df_meta['source_date']).dt.date)\n",
    "        dates_missing = set(dates[1:]) - src_dates\n",
    "        if dates_missing:\n",
    "            min_date = min(set(dates[1:]) - src_dates) - timedelta(days=1)\n",
    "            return_dates = [date.strftime(src_format) for date in dates if date >= min_date]\n",
    "            return_min_date = (min_date + timedelta(days=1)).strftime(src_format)\n",
    "        else:\n",
    "            return_dates = []\n",
    "            return_min_date = datetime(2200, 1, 1).date()\n",
    "    except bucket.session.client('s3').execptions.NoSuchKey:\n",
    "        return_dates = [(min_date + timedelta(days=x)).strftime(src_format) for x in range(0, (max_date-min_date).days + 1)]\n",
    "        return_min_date = arg_date\n",
    "    return return_min_date, return_dates\n",
    "\n",
    "def update_meta_file(bucket, meta_key, extract_date_list):\n",
    "    df_new = pd.DataFrame(columns=['source_date', 'datetime_of_processing'])\n",
    "    df_new['source_date'] = extract_date_list\n",
    "    df_new['datetime_of_processing'] = datetime.now().strftime(\"%Y-%m-$d %H:%M:%S\")\n",
    "    df_old = read_csv_to_df(bucket, meta_key)\n",
    "    df_all = pd.concat([df_old, df_new])\n",
    "    write_df_to_s3_csv(bucket, df_all, meta_key)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929b3aa2-0cff-4cf9-a447-4d6dc11ff4fe",
   "metadata": {},
   "source": [
    "# Main function entrypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "13a630ae-ee8b-4aad-82c6-85ffe258c4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Parameters/Configurations\n",
    "    # Later read config\n",
    "    arg_date = '2022-01-04'\n",
    "    src_format = '%Y-%m-%d'\n",
    "    src_bucket = 'xetra-1234'\n",
    "    trg_bucket = 'etl-pipelines-kspyrikos'\n",
    "    columns = ['ISIN', 'Date', 'Time', 'StartPrice', 'MaxPrice', 'MinPrice', 'EndPrice', 'TradedVolume']\n",
    "    trg_key = 'report_'\n",
    "    trg_format = '.parquet'\n",
    "    meta_key = 'meta_file.csv'\n",
    "    \n",
    "\n",
    "    # Init\n",
    "    s3 = boto3.resource('s3')\n",
    "    bucket_src = s3.Bucket(src_bucket)\n",
    "    bucket_trg = s3.Bucket(trg_bucket)\n",
    "   \n",
    "    # Run Application\n",
    "    extract_date, date_list = return_date_list(bucket_trg, arg_date, src_format, meta_key)\n",
    "    print(extract_date, date_list)\n",
    "    etl_report1(bucket_src, bucket_trg, date_list, columns, extract_date, trg_key, trg_format, meta_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b3dd20-b193-457f-840e-3a94104b542b",
   "metadata": {},
   "source": [
    "# Run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c9963c56-991f-4417-ba76-9660aa069635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-03 2022-01-11\n",
      "2022-01-05 ['2022-01-04', '2022-01-05', '2022-01-06', '2022-01-07', '2022-01-08', '2022-01-09', '2022-01-10', '2022-01-11']\n",
      "['2022-01-04', '2022-01-05', '2022-01-06', '2022-01-07', '2022-01-08', '2022-01-09', '2022-01-10', '2022-01-11']\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772b45cc-adc4-48dc-98a8-854bf272f9bb",
   "metadata": {},
   "source": [
    "# Read the uploaded file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6906da4-e4ca-4888-9623-5cf1155e03c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "trg_bucket = 'etl-pipelines-kspyrikos'\n",
    "s3 = boto3.resource('s3')\n",
    "bucket_trg = s3.Bucket(trg_bucket)\n",
    "   \n",
    "for obj in bucket_trg.objects.all():\n",
    "    print(obj.key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cf9bef-0a9e-4615-88ae-d13e5ea42bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj.key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91729dd8-353e-4bfe-99b1-bed374c3a306",
   "metadata": {},
   "outputs": [],
   "source": [
    "prq_obj = bucket_trg.Object(key=obj.key).get().get('Body').read()\n",
    "data = BytesIO(prq_obj)\n",
    "df_report = pd.read_parquet(data) # I can use 'read_parquet', because I used 'BytesIO'\n",
    "df_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4aea95d-9a2a-403d-a427-4b7128d1fd90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0970b648-2b69-49f0-9e4d-f6e5ae1cdd8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7846c2-6f40-4a0f-9dd3-6714c95b4dc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158a558e-8bcc-4361-b266-cc405381c4d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
